"""
Split reads by haplotype.

Reads FASTQ/BAM file and a list of haplotype assignments (such as generated by 
whatshap haplotag --output-haplotag-list). Outputs one FASTQ/BAM per haplotype.
BAM mode is intended for unmapped BAMs (such as provided by PacBio).
"""
import logging
import sys
import gzip
import pysam
from collections import defaultdict, Counter
from subprocess import Popen, PIPE

from contextlib import ExitStack
from whatshap import __version__
from whatshap.timer import StageTimer

logger = logging.getLogger(__name__)


def add_arguments(parser):
	arg = parser.add_argument
	arg('--output-h1', default=None,
		help='Output file to write reads from Haplotype 1 to. Use ending .gz to '
		'create gzipped file.')
	arg('--output-h2', default=None,
		help='Output file to write reads from Haplotype 2 to. Use ending .gz to '
		'create gzipped file.')
	arg('--output-untagged', default=None,
		help='Output file to write untagged reads to. Use ending .gz to '
		'create gzipped file.')
	arg('--add-untagged', default=False, action='store_true',
		help='Add reads without tag to both H1 and H2 output streams.')
	arg('--pigz', default=False, action='store_true',
		help='Use the pigz program for gzipping output.')
	arg('--only-largest-block', default=False, action='store_true',
		help='Only consider reads to be tagged if they belong to the largest '
		'phased block (in terms of read count) on their respective chromosome')
	arg('--discard-unknown-reads', default=False, action='store_true',
		help='Only check the haplotype of reads listed in the haplotag list file. '
			'Reads (read names) not contained in this file will be discarded. '
			'In the default case (= keep unknown reads), those reads would be '
			'considered untagged and end up in the respective output file. '
			'Please be sure that the read names match between the input FASTQ/BAM '
			'and the haplotag list file.')
	arg('--read-lengths-histogram', default=None,
		help='Output file to write read lengths histogram to in tab separated format.')
	arg('reads_file', metavar='READS', help='Input FASTQ/BAM file with reads (fastq can be gzipped)')
	arg('list_file', metavar='LIST',
		help='Tab-separated list with (at least) two columns <readname> and <haplotype> (can be gzipped). '
			'Currently, the two haplotypes have to be named H1 and H2 (or none). Alternatively, the '
			'output of the "haplotag" command can be used (4 columns), and this is required for using '
			'the "--only-largest-block" option (need phaseset and chromosome info).')


def validate(args, parser):
	if (args.output_h1 is None) and (args.output_h2 is None) and (args.output_untagged is None):
		parser.error('Nothing to be done since neither --output-h1 nor --output-h2 nor --output-untagged are given.')


def open_possibly_gzipped(filename, exit_stack, readwrite='r', pigz=False):
	"""
	TODO: this should be simplified after the the utils::detect_file_format
	function has been extended for more file formats

	TODO: the implicit dependency to the external tool pigz for faster I/O
	should be replaced with proper multi-threaded and buffered writers
	for the FASTQ output

	:param filename:
	:param exit_stack:
	:param readwrite:
	:param pigz:
	:return:
	"""
	if filename is None:
		# not sure why this is necessary - keep for the time being
		requested_file = None
	elif readwrite == 'r':
		if filename.endswith('.gz'):
			requested_file = exit_stack.enter_context(gzip.open(filename, 'rt'))
		else:
			requested_file = exit_stack.enter_context(open(filename, 'r'))
	elif readwrite == 'w':
		if filename.endswith('.gz'):
			if pigz:
				g = Popen(['pigz'], stdout=open(filename, 'w'), stdin=PIPE)
				requested_file = g.stdin
			else:
				requested_file = exit_stack.enter_context(gzip.open(filename, 'w'))
		else:
			requested_file = exit_stack.enter_context(open(filename, 'wb'))
	else:
		raise ValueError('Invalid file open mode (must be "r" or "w"): {}'.format(readwrite))
	return requested_file


def read_fastq(filename):
	'''Yields pairs (readname, record) where record is a list of four lines.'''
	f = open_possibly_gzipped(filename)
	n = 0
	while True:
		record = [ f.readline() for _ in range(4) ]
		if record[3] == '':
			break
		assert record[0].startswith('@'), record
		assert record[2].startswith('+'), record
		name = record[0][1:].split()[0]
		yield name, record


def select_reads_in_largest_phased_blocks(block_sizes, block_to_readnames):
	"""
	:param block_sizes:
	:param block_to_readnames:
	:return:
	"""
	selected_reads = set()
	logger.info('Determining largest blocks/phasesets per chromosome')
	for chromosome, block_counts in block_sizes.items():
		block_name, reads_in_block = block_counts.most_common(1)[0]
		logger.info(
			'Chromosome: {} - Phaseset: {} - Tagged reads: {}'.format(
				chromosome, block_name, reads_in_block
			)
		)
		selected_reads = selected_reads.union(set(block_to_readnames[(chromosome, block_name)]))
	logger.info('Total number of haplo-tagged reads in all largest phased blocks: {}'.format(len(selected_reads)))
	return selected_reads


def process_haplotag_list_file(haplolist, line_parser, only_largest_blocks, discard_unknown_reads):
	"""
	:param haplolist:
	:param line_parser:
	:param only_largest_blocks:
	:param discard_unknown_reads:
	:return:
	"""
	# TODO: obviously this won't work for more than two haplotypes
	haplotype_to_int = {'none': 0, 'H1': 1, 'H2': 2}

	is_header = haplolist.readline().startswith('#')
	if not is_header:
		haplolist.seek(0)

	# needed to determine largest phased block
	block_sizes = defaultdict(Counter)
	# for later removal of reads not in largest phased block;
	# since this can grow quite a bit, only fill if needed
	blocks_to_readnames = defaultdict(set)

	# this set should not be too large given
	# that the haplotag list file contains only
	# a subset of the reads in the input FASTQ/BAM
	known_reads = set()

	readname_to_haplotype = defaultdict(int)
	total_reads = 0

	for line in haplolist:
		readname, haplo_name, phaseset, chromosome = line_parser(line)
		total_reads += 1
		haplo_num = haplotype_to_int[haplo_name]
		if haplo_num == 0:
			if discard_unknown_reads:
				known_reads.add(readname)
			# Some "trickery" here:
			# Haplotype 0 means untagged;
			# the return value of a defaultdict(int)
			# is zero for unknown keys, so no need to store
			# anything unless "--discard-unknown-reads" is True,
			# in which case we need to know all read names
			continue
		readname_to_haplotype[readname] = haplo_num
		if only_largest_blocks:
			block_sizes[chromosome][phaseset] += 1
			blocks_to_readnames[(chromosome, phaseset)].add(readname)

	tagged_reads = len(readname_to_haplotype)
	untagged_reads = total_reads - tagged_reads
	logger.info('Total number of reads in haplotag list: {}'.format(total_reads))
	logger.info('Total number of haplo-tagged reads: {}'.format(tagged_reads))
	logger.info('Total number of untagged reads: {}'.format(untagged_reads))

	if discard_unknown_reads:
		known_reads = known_reads.union(set(readname_to_haplotype.keys()))
		num_known_reads = len(known_reads)
		assert total_reads == num_known_reads, \
			'Mismatch between total number of reads and known reads: {} vs {}'.format(total_reads, num_known_reads)

	if only_largest_blocks:
		selected_reads = select_reads_in_largest_phased_blocks(
			block_sizes,
			blocks_to_readnames
		)
		readname_to_haplotype = defaultdict(int, {k: readname_to_haplotype[k] for k in selected_reads})
		num_removed_reads = total_reads - len(readname_to_haplotype)
		logger.info('Number of reads removed / '
					'reads not overlapping largest phased blocks: {}'.format(num_removed_reads))

	return readname_to_haplotype, known_reads


def _two_column_parser(line):
	cols = line.strip().split('\t')[:2]
	return cols[0], cols[1], None, None


def _four_column_parser(line):
	return line.strip().split('\t')[:4]


def check_haplotag_list_information(haplotag_list, exit_stack):
	"""
	Check if the haplotag list file has at least 4 columns
	(assumed to be read name, haplotype, phaseset, chromosome),
	or at least 2 columns (as above). Fails if the haplotag file
	is not tab-separated. Return suitable parser for format

	:param haplotag_list: Tab-separated file with at least 2 or 4 columns
	:param exit_stack:
	:return:
	"""
	haplo_list = open_possibly_gzipped(haplotag_list, exit_stack)
	first_line = haplo_list.readline().strip()
	# rewind to make sure a header-less file is processed correctly
	haplo_list.seek(0)
	has_chrom_info = False
	try:
		_, _, _, _ = first_line.split('\t')[:4]
		line_parser = _four_column_parser
	except ValueError:
		try:
			_, _ = first_line.split('\t')[:2]
			line_parser = _two_column_parser
		except ValueError:
			raise ValueError('First line of haplotag list file does not have '
							'at least 2 columns, or it is not tab-separated: {}'.format(first_line))
	else:
		has_chrom_info = True
	return haplo_list, has_chrom_info, line_parser


def run_split(
		reads_file,
		list_file,
		output_h1=None,
		output_h2=None,
		output_untagged=None,
		add_untagged=False,
		pigz=False,
		only_largest_block=False,
		discard_unknown_reads=False,
		read_lengths_histogram=None,
	):

	timers = StageTimer()
	timers.start('split-run')

	with ExitStack() as stack:
		timers.start('split-init')

		haplo_list, has_haplo_chrom_info, line_parser = check_haplotag_list_information(list_file, stack)

		if only_largest_block:
			logger.debug('User selected "--only-largest-block", this requires chromosome '
						'and phaseset information to be present in the haplotag list file.')
			if not has_haplo_chrom_info:
				raise ValueError('The haplotag list file does not contain phaseset and chromosome '
								'information, which is required to select only reads from the '
								'largest phased block. Columns 3 and 4 are missing.')

		readname_to_haplotype, known_reads = process_haplotag_list_file(
			haplo_list,
			line_parser,
			only_largest_block,
			discard_unknown_reads
		)

		output_h1_file = None
		output_h2_file = None
		output_untagged_file = None

		read_lengths_histogram_dict = dict()

		n_reads = 0
		# TODO: Avoid code duplication in the two code blocks
		# TODO: Detect file type in a smarter way, not based on ending
		if reads_file.endswith('.bam'):
			bamreader = pysam.AlignmentFile(reads_file, mode='rb', check_sq=False)
			if output_h1 is not None:
				output_h1_file = pysam.AlignmentFile(output_h1, 'wb', template=bamreader)
			if output_h2 is not None:
				output_h2_file = pysam.AlignmentFile(output_h2, 'wb', template=bamreader)
			if output_untagged is not None:
				output_untagged_file = pysam.AlignmentFile(output_untagged, 'wb', template=bamreader)

			for record in bamreader:
				n_reads += 1
				h = readname_to_haplotype[record.query_name]

				if read_lengths_histogram is not None:
					read_length = len(record.query_sequence)
					if read_length not in read_lengths_histogram_dict:
						read_lengths_histogram_dict[read_length] = [0,0,0]
					read_lengths_histogram_dict[read_length][h] += 1

				if output_h1_file is not None:
					if (h==1) or (h==0 and add_untagged):
						output_h1_file.write(record)

				if output_h2_file is not None:
					if (h==2) or (h==0 and add_untagged):
						output_h2_file.write(record)

				if output_untagged_file is not None:
					if h==0:
						output_untagged_file.write(record)

		else:
			output_h1_file = open_possibly_gzipped(output_h1, stack, 'w', pigz)
			output_h2_file = open_possibly_gzipped(output_h2, stack, 'w', pigz)
			output_untagged_file = open_possibly_gzipped(output_untagged, stack, 'w', pigz)

			for name, record in read_fastq(reads_file):
				n_reads += 1
				h = readname_to_haplotype[name]

				if read_lengths_histogram is not None:
					read_length = len(record[1].strip())
					if read_length not in read_lengths_histogram_dict:
						read_lengths_histogram_dict[read_length] = [0,0,0]
					read_lengths_histogram_dict[read_length][h] += 1

				if output_h1_file is not None:
					if (h==1) or (h==0 and add_untagged):
						for line in record:
							output_h1_file.write(line.encode('utf-8'))

				if output_h2_file is not None:
					if (h==2) or (h==0 and add_untagged):
						for line in record:
							output_h2_file.write(line.encode('utf-8'))

				if output_untagged_file is not None:
					if h==0:
						for line in record:
							output_untagged_file.write(line.encode('utf-8'))

		if read_lengths_histogram is not None:
			with open(read_lengths_histogram, 'wt') as t:
				print('#length', 'count-untagged', 'count-h1', 'count-h2', sep='\t', file=t)
				for length in sorted(read_lengths_histogram_dict.keys()):
					print(length, *(read_lengths_histogram_dict[length]), sep='\t', file=t)
				t.close()

		if output_h1_file is not None:
			output_h1_file.close()
		if output_h2_file is not None:
			output_h2_file.close()
		if output_untagged_file is not None:
			output_untagged_file.close()

	logger.info('\n== SUMMARY ==')
	logger.info('Total reads processed:              %12d', n_reads)


def main(args):
	run_split(**vars(args))
